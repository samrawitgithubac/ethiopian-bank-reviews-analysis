Task 1 â€” Data Collection & Preprocessing (Week 2 Challenge)
Customer Experience Analytics for Fintech Apps â€” Google Play Review Scraping

This repository contains the implementation for Task 1 of the Week 2 Omega Consultancy Data Challenge.
The goal of this task is to scrape Google Play Store reviews for three Ethiopian banking apps, clean the data, and prepare it for sentiment and thematic analysis.

ğŸš€ 1. Objective

The main goal of Task 1 is to:

âœ” Scrape 400+ reviews per bank from Google Play
âœ” Preprocess and clean the dataset
âœ” Save a combined CSV file with all banks
âœ” Maintain clean Git workflow (task-1 branch)

This task lays the foundation for Task 2 (NLP), Task 3 (Database), and Task 4 (Insights).

ğŸ¦ 2. Target Banking Apps

We scrape reviews for the following banks:

Bank	Google Play App Name	Example Package ID
Commercial Bank of Ethiopia	CBE Mobile Banking	com.cbe.eBanking
Bank of Abyssinia	BOA Mobile Banking	com.bankOfAbyssinia.mobilebanking
Dashen Bank	Amole / Dashen Mobile Banking	com.dashen.mobilebanking

ğŸ“Œ Exact package IDs may differ â€” verify from Google Play Store before scraping.

ğŸ› ï¸ 3. Technologies Used
Tool	Purpose
google-play-scraper	Scraping reviews
Python	Main programming language
Pandas	Data cleaning & preprocessing
Git & GitHub	Version control
Virtual Environment	Dependency isolation
ğŸ“¦ 4. Project Setup
4.1 Clone the Repository
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>

4.2 Create & Activate Virtual Environment
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

4.3 Install Dependencies
pip install -r requirements.txt

4.4 Install Scraper
pip install google-play-scraper

ğŸ“¥ 5. Scraping Script

The main script for scraping reviews is:

ğŸ“„ scrape_reviews.py

This script:

âœ” Scrapes reviews (ratings, dates, content)
âœ” Saves data to Pandas DataFrame
âœ” Adds an additional column bank
âœ” Merges all banks into one CSV file

Example code block used:
from google_play_scraper import reviews
import pandas as pd

def scrape_bank(app_id, bank_name, count=500):
    data, _ = reviews(
        app_id,
        lang="en",
        country="us",
        count=count,
        sort=reviews.Sort.NEWEST
    )

    df = pd.DataFrame(data)
    df["bank"] = bank_name
    df["source"] = "Google Play"

    return df

ğŸ§¹ 6. Preprocessing Steps

The cleaning script is located in:

ğŸ“„ preprocess_reviews.py

Cleaning Includes:

âœ” Remove duplicate reviews
âœ” Drop empty or missing review text
âœ” Convert dates to format YYYY-MM-DD
âœ” Keep relevant columns only:

review_text  
rating  
date  
bank  
source  

ğŸ“ 7. Output Files

The final cleaned dataset is saved as:

ğŸ“„ cleaned_reviews.csv

This file contains:

Column	Description
review	The review text
rating	Star rating (1â€“5)
date	Normalized date
bank	Bank name
source	â€œGoogle Playâ€

Minimum rows required â†’ 1200+ reviews.

ğŸ”€ 8. Git Workflow for Task 1

Work was done on a dedicated branch:

git checkout -b task-1


After completing scraping + preprocessing:

git add .
git commit -m "Completed Task 1 - scraping and preprocessing"
git push origin task-1


Then a Pull Request was created and merged to main.

âœ… 9. Deliverables Completed
âœ” GitHub repo created
âœ” Clean folder structure
âœ” Scraper implemented
âœ” Preprocessing completed
âœ” Clean CSV generated
âœ” README updated
ğŸ“š 10. How to Run Entire Task 1
python scrape_reviews.py
python preprocess_reviews.py


This will generate:

raw_reviews.csv
cleaned_reviews.csv